import os
import json
import click
from rich.console import Console
from agent.graph_hybrid import build_graph
import dspy


console = Console()

def load_questions(batch_file: str) -> list:
    """Loads questions from a JSONL file."""
    questions = []
    try:
        with open(batch_file, 'r') as f:
            for line in f:
                questions.append(json.loads(line))
    except FileNotFoundError:
        console.print(f"[bold red]Error:[/bold red] Batch file not found at {batch_file}")
        exit(1)
    return questions

def run_agent(question_data: dict, app) -> dict:
    """Runs the LangGraph agent for a single question."""
    question_id = question_data["id"]
    question = question_data["question"]
    
    console.print(f"\n[bold blue]Processing Question ID:[/bold blue] {question_id}")
    console.print(f"[bold blue]Question:[/bold blue] {question}")
    
    initial_state = {
        "question": question,
        "repair_count": 0
    }
    
    try:
        # The actual graph execution logic will be more complex
        # For now, we just invoke it and get a placeholder result
        result_state = app.invoke(initial_state)
        
        # This is a placeholder for the final output structure
        final_answer = result_state.get("final_answer", "No answer generated.")
        sql_query = result_state.get("sql_query", "")
        citations = result_state.get("citations", [])
        
        result = {
            "id": question_id,
            "final_answer": final_answer,
            "sql": sql_query,
            "confidence": 0.85,  # Placeholder
            "explanation": "Answer generated by the agent.", # Placeholder
            "citations": citations
        }
        
        console.print(f"[bold green]Final Answer:[/bold green] {result['final_answer']}")
        console.print(f"[bold green]SQL:[/bold green] {result['sql']}")
        console.print(f"[bold green]Citations:[/bold green] {', '.join(result['citations'])}")
        
        return result

    except Exception as e:
        console.print(f"[bold red]Critical Error for {question_id}:[/bold red] {e}")
        return {
            "id": question_id,
            "final_answer": f"CRITICAL ERROR: {e}",
            "sql": "",
            "confidence": 0.0,
            "explanation": "Critical error during graph execution.",
            "citations": []
        }

@click.command()
@click.option('--batch', required=True, help='Path to the JSONL file containing batch questions.')
@click.option('--out', required=True, help='Path to the output JSONL file.')
def main(batch: str, out: str):
    """
    Retail Analytics Copilot: A hybrid RAG/SQL agent built with DSPy and LangGraph.
    """
    console.print("[bold yellow]Initializing Agent...[/bold yellow]")
    
    # 1. Configure DSPy Language Model (Bypassed due to LLM credit error)
    # dspy.configure(lm=dspy.OpenAI(model='gpt-4.1-mini'))
    # console.print("[bold green]DSPy Language Model configured.[/bold green]")
    
    # 1. Build the graph
    app = build_graph()
    
    # 2. Load questions
    questions = load_questions(batch)
    
    # 3. Process questions
    results = []
    for question_data in questions:
        result = run_agent(question_data, app)
        results.append(result)
        
    # 4. Write output
    with open(out, 'w') as f:
        for result in results:
            f.write(json.dumps(result) + '\n')
            
    console.print(f"\n[bold green]Processing Complete.[/bold green] Results written to {out}")

if __name__ == '__main__':
    main()
